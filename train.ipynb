class NGramInterpolation(object):
    def __init__(self, n, lambda_list, vsize):
        self.n = n
        self.delta = delta
        self.count = defaultdict(lambda: defaultdict(float))
        self.total = defaultdict(float)
        self.vsize = vsize
        self.lambda_list = lambda_list
        self.ovw = 0 #out of vocabulary word
        self.delta = 0.005
    
    def estimate(self, sequences):
        for sequence in sequences:
            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']
            for i in range(len(padded_sequence) - self.n+1):
                ngram = tuple(padded_sequence[i:i+self.n])
                prefix, word = ngram[:-1], ngram[-1]
                self.count[prefix][word] += 1
                self.total[prefix] += 1
                
    def sequence_logp(self, sequence):
        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']
        total_logp = 0
        for i in range(len(padded_sequence) - self.n+1):
            ngram = tuple(padded_sequence[i:i+self.n])
            total_logp += np.log2(self.ngram_prob(ngram))
        return total_logp

    def ngram_prob(self, ngram):
        
        index = 0
        prob = 0
        
        while index<self.n:
            if self.total[ngram[:-1]]!=0:
                prob+=lambda_list[0]*(self.count[ngram[index:-1]][ngram[-1]])/(self.total[ngram[:-1]])
            index+=1
        if prob == 0: #use additive soothing with delta = 0.005 for out of vocabulary word
            prob = (self.count[ngram[index:-1]][ngram[-1]]+self.delta)/(self.total[ngram[index:-1]]+self.delta*self.vsize) 
        return prob
        
        
        
#run the loop 100 times to catch the best parameters
optim_val_perp = 100000000000000
val_perp_list_4 = []
train_perp_list_4 = []
for i in range(100):
    #randomly generate lambda_list
    a = np.random.random(4)
    a /= a.sum()
    
    lambda_list = a
    #print('weight:')
    #print(a)

    lm = NGramInterpolation(n=4, lambda_list=lambda_list, vsize=len(vocab)+1)  # +1 is for <eos>
    lm.estimate(datasets['train'])
    
    train_perp = perplexity(lm, datasets['train'])
    val_perp = perplexity(lm, datasets['valid'])
    
    val_perp_list_4.append(val_perp)
    train_perp_list_4.append(train_perp)
    

    #print("Train Perplexity: %.3f" % (train_perp))
    #print("Valid Perplexity: %.3f" % (val_perp))
    #print(i)
    if val_perp < optim_val_perp:
        optim_val_perp = val_perp
        optim_lambda = lambda_list
print('resulted optimum valuation perplexity and the corresponding lambda')
print(optim_val_perp)    
print(optim_lambda)
